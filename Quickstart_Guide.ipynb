{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw4c0esr4QYXlq4/nOyYpP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TirendazAcademy/LangChain-Tutorials/blob/main/Quickstart_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Tutorial"
      ],
      "metadata": {
        "id": "5wjtV2JN9Cvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial walks you through how to use LangChain."
      ],
      "metadata": {
        "id": "qoLu0W0H_oGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "1pBexl6Y_fx8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCvj819Yf8tu"
      },
      "outputs": [],
      "source": [
        "!pip install langchain \n",
        "!pip install openai \n",
        "!pip install google-search-results "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Necessary Libraries"
      ],
      "metadata": {
        "id": "SVcHX_dW_jxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain import ConversationChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import (PromptTemplate, \n",
        "                               ChatPromptTemplate, \n",
        "                               MessagesPlaceholder, \n",
        "                               SystemMessagePromptTemplate, \n",
        "                               HumanMessagePromptTemplate)\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.agents import (load_tools, \n",
        "                              initialize_agent, \n",
        "                              AgentType)\n",
        "from langchain.schema import (AIMessage, \n",
        "                              HumanMessage, \n",
        "                              SystemMessage)\n",
        "from langchain.prompts.chat import (ChatPromptTemplate, \n",
        "                                    SystemMessagePromptTemplate,\n",
        "                                    HumanMessagePromptTemplate)\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "yXIA5lVtgT6z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "vgAQvzaq9XdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"...\""
      ],
      "metadata": {
        "id": "qX-No2tp9ZXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMs: Get predictions from a language model"
      ],
      "metadata": {
        "id": "kCyGJkdR9izy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "wWWtvD4NqqLT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Suggest a youtube name for AI\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqdjoc3jq0J6",
        "outputId": "86d34972-e239-4d48-efbb-7eed1a4b3444"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "AI Wiz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates: Manage prompts for LLMs"
      ],
      "metadata": {
        "id": "1QwCEOev9oQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Recommend a youtube name for {topic}?\",\n",
        ")"
      ],
      "metadata": {
        "id": "eVniyzSzuzWd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(topic=\"AI\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrF0PjyJvLGi",
        "outputId": "4e3cbfa3-bd9f-4285-98c9-cefe482c9f07"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommend a youtube name for AI?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "KGzNVlL90yuh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"data science\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qDKcioiO096W",
        "outputId": "023cd793-6cc8-4451-d1ac-1a466f5d149b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nDataScienceTube'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents: Dynamically Call Chains Based on User Input"
      ],
      "metadata": {
        "id": "YJvBK6jv2gW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's load the language model we're going to use to control the agent.\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "CF46aDoU2y3d",
        "outputId": "79ed61ea-5a2f-4214-89fc-3322c9aa0169"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
            "Action: Search\n",
            "Action Input: \"High temperature in SF yesterday\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mHigh: 69.8ºf @3:45 PM Low: 55.04ºf @5:56 AM Approx.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to use the calculator to raise 69.8 to the .023 power\n",
            "Action: Calculator\n",
            "Action Input: 69.8^.023\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.1025763550530143\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: 1.1025763550530143\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.1025763550530143'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory: Add State to Chains and Agents"
      ],
      "metadata": {
        "id": "i6pY8bCd3gEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "output = conversation.predict(input=\"Hi there!\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH13BtWd3hDV",
        "outputId": "8ac609c8-4cf1-4c16-d0ee-678f6ed5ed8d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Hi there! It's nice to meet you. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D-8v4Ks5JAs",
        "outputId": "d6170741-0391-480e-97e5-4d41f837ffde"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Message Completions from a Chat Model"
      ],
      "metadata": {
        "id": "zFzRZipF5bAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "W2VjRzo35Ri1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat([HumanMessage(content=\"Translate this sentence from English to Turkish. I love programming.\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YwJK9Kd5r1d",
        "outputId": "a8cd8649-b2bd-46ee-e605-ae8f213121d0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 618b332d89201832b2c4c918db5a7651 in your message.).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(lc_kwargs={'content': 'Ben programlamayı seviyorum.'}, content='Ben programlamayı seviyorum.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to Turkish.\"),\n",
        "    HumanMessage(content=\"I love programming.\")\n",
        "]\n",
        "chat(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMx4rU_J5vXU",
        "outputId": "406f7a92-e5f2-4a27-f6a8-068862027b1a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(lc_kwargs={'content': 'Ben programlamayı seviyorum.'}, content='Ben programlamayı seviyorum.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_messages = [\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "        HumanMessage(content=\"I love programming.\")\n",
        "    ],\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "        HumanMessage(content=\"I love artificial intelligence.\")\n",
        "    ],\n",
        "]\n",
        "result = chat.generate(batch_messages)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBfdrMQA6a4L",
        "outputId": "6d51c154-f1c7-4e97-e5fb-f7996abab77a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[ChatGeneration(lc_kwargs={'message': AIMessage(lc_kwargs={'content': \"J'adore la programmation.\"}, content=\"J'adore la programmation.\", additional_kwargs={}, example=False)}, text=\"J'adore la programmation.\", generation_info=None, message=AIMessage(lc_kwargs={'content': \"J'adore la programmation.\"}, content=\"J'adore la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(lc_kwargs={'message': AIMessage(lc_kwargs={'content': \"J'adore l'intelligence artificielle.\"}, content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}, example=False)}, text=\"J'adore l'intelligence artificielle.\", generation_info=None, message=AIMessage(lc_kwargs={'content': \"J'adore l'intelligence artificielle.\"}, content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo'}, run=RunInfo(run_id=UUID('cd379524-dc22-475c-ba5f-db2ef7722036')))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.llm_output['token_usage']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8JG9eQr6iO9",
        "outputId": "b0c229a0-8238-48de-daa9-acaf94a9ddd0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Prompt Templates"
      ],
      "metadata": {
        "id": "HSaHRDiz6pLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# get a chat completion from the formatted messages\n",
        "chat(chat_prompt.format_prompt(input_language=\"English\", \n",
        "                               output_language=\"Turkish\", \n",
        "                               text=\"I love programming.\").to_messages())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FbafofP6mgD",
        "outputId": "f2cb965e-d2d6-432e-9728-2db6b65ee250"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(lc_kwargs={'content': 'Ben programlamayı seviyorum.'}, content='Ben programlamayı seviyorum.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains with Chat Models"
      ],
      "metadata": {
        "id": "c8xDm2_J7IGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "chain.run(input_language=\"English\", output_language=\"Turkish\", text=\"I love programming.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a8mX8BGO7JJb",
        "outputId": "726ba550-83df-4c44-c903-43eebb6b202a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ben programlamayı seviyorum.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents with Chat Models"
      ],
      "metadata": {
        "id": "JYvB5BdY7Yps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's load the language model we're going to use to control the agent.\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "llm = OpenAI(temperature=0)\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "k886YmjY7aEp",
        "outputId": "70b88cc3-a46c-41e1-e966-cefbac7767f1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Search\",\n",
            "  \"action_input\": \"Olivia Wilde boyfriend\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mLooks like Olivia Wilde and Jason Sudeikis are starting 2023 on good terms. Amid their highly publicized custody battle – and the actress' ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mNow I need to use a calculator to raise Jason Sudeikis' age to the 0.23 power.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": \"47^0.23\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.4242784855673896\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know that Jason Sudeikis' current age raised to the 0.23 power is 2.4242784855673896.\n",
            "Final Answer: Jason Sudeikis' current age raised to the 0.23 power is 2.4242784855673896.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Jason Sudeikis' current age raised to the 0.23 power is 2.4242784855673896.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory: Add State to Chains and Agents"
      ],
      "metadata": {
        "id": "emKLYRiz8Aws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\n",
        "        \"The following is a friendly conversation between a human and an AI.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")\n",
        "# -> 'Hello! How can I assist you today?'\n",
        "\n",
        "\n",
        "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
        "# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\n",
        "\n",
        "conversation.predict(input=\"Tell me about yourself.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "PC9zwbTH8DLB",
        "outputId": "276e0f98-1bd2-40a8-e788-2dbd0c1f7e71"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an AI language model designed to assist with various tasks such as answering questions, generating text, and providing recommendations. I am constantly learning and improving my abilities through machine learning algorithms. Is there anything specific you would like to know about me?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resource\n",
        "\n",
        "- [LangChain Doc](https://python.langchain.com/en/latest/getting_started/getting_started.html#environment-setup)"
      ],
      "metadata": {
        "id": "L6HvwP4h_7Zx"
      }
    }
  ]
}